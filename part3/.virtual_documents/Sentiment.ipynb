import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns



sentiment_df = pd.read_csv("./sentiment140_cleaned.csv")


sentiment_df.head()


sentiment_df.shape


sentiment_df.info()


sentiment_df.describe()


sentiment_df[sentiment_df.cleaned_text.isna() == True]


sentiment_df['cleaned_text'] = sentiment_df['cleaned_text'].fillna("").astype(str)


sentiment_df.target.value_counts()


sentiment_df[sentiment_df.target == 0].head(1)


sentiment_df[sentiment_df.target == 1].head(1)


# 1 -> Positive, 0 -> Negative


y = sentiment_df['target'].values





from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(max_features=1000)  # limit vocab for simplicity
X_bow = vectorizer.fit_transform(sentiment_df['cleaned_text']).toarray()


X_bow.shape


from sklearn.model_selection import train_test_split

X_train_full, X_test, y_train_full, y_test = train_test_split(
    X_bow, y, test_size=0.2, random_state=42
)
X_train, X_val, y_train, y_val = train_test_split(
    X_train_full, y_train_full, test_size=0.2, random_state=42
)



import tensorflow as tf
from tensorflow.keras import layers, models

input_dim = X_train.shape[1]

model = models.Sequential([
    layers.Input(shape=(input_dim,)),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(1, activation='sigmoid')
])
# raytune

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])


history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=15,
    batch_size=32,
    verbose=1
)


test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
print(test_acc)


import joblib
model.save("ffnn_bow_model.keras")
joblib.dump(vectorizer, "bow_vectorizer.pkl")
# model = tf.keras.models.load_model("ffnn_bow_model.keras")
# vectorizer = joblib.load(vectorizer, "bow_vectorizer.pkl")


plt.figure(figsize=(10,4))

# Plot accuracy
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.title('Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Plot loss
plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Loss')
plt.xlabel('Epochs')
plt.ylabel('Binary Crossentropy')
plt.legend()

plt.show()






from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer(max_features=1000)

X = tfidf.fit_transform(sentiment_df['cleaned_text']).toarray()
y = sentiment_df['target'].values

print("TF-IDF shape:", X.shape)


from sklearn.model_selection import train_test_split

X_train_full, X_test, y_train_full, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
X_train, X_val, y_train, y_val = train_test_split(
    X_train_full, y_train_full, test_size=0.2, random_state=42
)


import tensorflow as tf
from tensorflow.keras import layers, models

input_dim = X_train.shape[1]

model = models.Sequential([
    layers.Input(shape=(input_dim,)),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=15,
    batch_size=32,
    verbose=1
)



test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
print(test_acc)


import joblib

# Save model
model.save("ffnn_tfidf_model.keras")

# Save vectorizer
joblib.dump(tfidf, "tfidf_vectorizer.pkl")



import matplotlib.pyplot as plt

plt.figure(figsize=(10,4))

plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.title('Accuracy')
plt.xlabel('Epochs')
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Loss')
plt.xlabel('Epochs')
plt.legend()

plt.show()






!pip install transformers torch --quiet


!pip install tf-keras


!pip install tensorflow-cpu==2.16.1
!pip install tf-keras==2.16.0 --no-dependencies


!pip uninstall keras --y



!pip install "keras<3"


from transformers import pipeline


texts = sentiment_df['cleaned_text'].tolist()
y = sentiment_df['target'].values

embedder = pipeline("feature-extraction", model="bert-base-uncased", device=-1)  # device=0 for GPU
embeddings = embedder(texts)


# Convert token embeddings â†’ sentence embeddings (mean pooling)
X = np.array([np.mean(e, axis=0) for e in embeddings])
print("BERT sentence embeddings shape:", X.shape)  


X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)


input_dim = X_train.shape[1]

model = models.Sequential([
    layers.Input(shape=(input_dim,)),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(1, activation='sigmoid')
])



model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=15,
    batch_size=4,
    verbose=1
)



test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
print(f"Test Accuracy: {test_acc:.4f}")


from transformers import pipeline, BertModel, BertTokenizer
# from tensorflow.keras.models import load_model

model.save("ffnn_bert_model.keras")

embedder.model.save_pretrained("bert_model")
embedder.tokenizer.save_pretrained("bert_model")

# loaded_model = load_model("ffnn_bert_model.keras")
# tokenizer = BertTokenizer.from_pretrained("bert_model")
# bert_model = BertModel.from_pretrained("bert_model")
# loaded_embedder = pipeline("feature-extraction", model=bert_model, tokenizer=tokenizer)


plt.figure(figsize=(10,4))

plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.title('Accuracy')
plt.xlabel('Epochs')
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Loss')
plt.xlabel('Epochs')
plt.legend()

plt.show()




